{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization in Machine Learning using Adagrad\n",
        "\n",
        "# Learning Objective\n",
        "\n",
        "Understand how optimization algorithms like **Adagrad** are used to train machine learning models, focusing on a **linear regression task**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Problem Overview\n",
        "\n",
        "**Goal**: Learn a simple linear relationship between hours studied (`x`) and student grades (`y`).\n",
        "\n",
        "Model:\n",
        "\n",
        "$$\n",
        "\\hat{y} = wx + b\n",
        "$$\n",
        "\n",
        "We aim to find the best parameters $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Core Concepts (coming from calculus)\n",
        "\n",
        "#### 1. **Gradient Descent Optimization**\n",
        "\n",
        "We use gradients (partial derivatives of the loss function) to update parameters:\n",
        "\n",
        "* Gradient w\\.r.t. weight $w$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{MSE}}{\\partial w} = \\frac{-2}{n} \\sum (y_i - \\hat{y}_i) x_i\n",
        "$$\n",
        "\n",
        "* Gradient w\\.r.t. bias $b$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{MSE}}{\\partial b} = \\frac{-2}{n} \\sum (y_i - \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "#### 2. **Adagrad Optimizer**\n",
        "\n",
        "Traditional gradient descent uses a fixed learning rate. Adagrad adapts the learning rate **individually for each parameter**, which:\n",
        "\n",
        "* Speeds up learning for infrequent features\n",
        "* Slows down learning for frequently updated parameters\n",
        "\n",
        "Update rule:\n",
        "\n",
        "$$\n",
        "\\theta := \\theta - \\frac{\\text{lr}}{\\sqrt{G_\\theta + \\epsilon}} \\cdot \\frac{\\partial L}{\\partial \\theta}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $G_\\theta$ = accumulated sum of squared gradients\n",
        "* $\\epsilon$ = small constant to avoid division by zero\n",
        "\n",
        "---\n",
        "\n",
        "### Code Walkthrough Highlights\n",
        "\n",
        "| Code Segment                                   | Concept Taught                          |\n",
        "| ---------------------------------------------- | --------------------------------------- |\n",
        "| `x`, `y`                                       | Feature and label vectors               |\n",
        "| `w = 0.0`, `b = 0.0`                           | Parameter initialization                |\n",
        "| `for epoch in range(epochs)`                   | Iterative training over multiple passes |\n",
        "| `y_pred = w * x + b`                           | Model prediction                        |\n",
        "| `dw`, `db`                                     | Gradient computation for MSE loss       |\n",
        "| `gw_accum += dw**2`                            | Accumulate squared gradients for weight |\n",
        "| `w -= (lr / np.sqrt(gw_accum + epsilon)) * dw` | Adagrad update for weight               |\n",
        "| `print(...)`                                   | Final model parameters                  |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "j0uDn-gXy4Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Import NumPy library for numerical operations\n",
        "\n",
        "# Dataset, x is the number of study hours, and y is the grades\n",
        "x = np.array([1, 2, 3, 4], dtype=np.float64)  # Input data (features)\n",
        "y = np.array([2, 4, 5, 7], dtype=np.float64)  # Output data (labels)"
      ],
      "metadata": {
        "id": "0hGlg6ho0mTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "w = 0.0  # Initial weight\n",
        "b = 0.0  # Initial bias"
      ],
      "metadata": {
        "id": "xqCbkS9N0mWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# learning rate: Controls the step size during parameter updates.\n",
        "A smaller value leads to slower but potentially more stable learning."
      ],
      "metadata": {
        "id": "t6R5HEuD02TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1  # Learning rate for gradient descent"
      ],
      "metadata": {
        "id": "dJBkwqfn1ZKx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100  # Number of training iterations\n",
        "epsilon = 1e-8  # Small value to prevent division by zero"
      ],
      "metadata": {
        "id": "yNoehO4d1imZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- gw_accum and gb_accum: Variables used to store the accumulated squared gradients for the weight and bias, respectively.\n",
        "These are crucial for the Adagrad optimization algorithm.\n",
        "\n",
        "- The purpose of accumulating squared gradients (gw_accum and gb_accum)\n",
        "in the Adagrad optimization algorithm is to adaptively adjust the learning rate\n",
        "for each parameter based on the past gradients.\n",
        "- By accumulating the square of gradients during training,\n",
        "Adagrad gives more weight to infrequent parameters by decreasing\n",
        " the learning rate for parameters that have large accumulated gradients.\n",
        "\n",
        "- This adaptive learning rate mechanism allows Adagrad to effectively handle\n",
        "sparse data and converge faster by mitigating the problem of choosing a\n",
        "global learning rate that may be too large or too small for different\n",
        "parameters in the model."
      ],
      "metadata": {
        "id": "shlKN0J11jxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accumulated squared gradients for Adagrad\n",
        "gw_accum = 0.0  # Accumulated squared gradient for weight\n",
        "gb_accum = 0.0  # Accumulated squared gradient for bias\n",
        "\n",
        "n = len(x)  # Number of data points"
      ],
      "metadata": {
        "id": "btm4ELhW2Bwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):  # Loop over specified number of epochs\n",
        "    # Predictions\n",
        "    y_pred = w * x + b  # Calculate predicted output using current parameters\n",
        "\n",
        "    # Compute gradients (MSE loss)\n",
        "    dw = (-2 / n) * np.sum((y - y_pred) * x)  # Gradient of weight\n",
        "    db = (-2 / n) * np.sum(y - y_pred)  # Gradient of bias\n",
        "\n",
        "    # Accumulate squared gradients\n",
        "    gw_accum += dw ** 2  # Accumulate squared gradient for weight\n",
        "    gb_accum += db ** 2  # Accumulate squared gradient for bias\n",
        "    '''\n",
        "These two lines are used to accumulate squared gradients for Adagrad optimization algorithm.\n",
        "\n",
        "In Adagrad, the accumulation of squared gradients is used to re-scale\n",
        "the learning rate individually for each parameter. This helps to handle sparse\n",
        "features or parameters that require different learning rates for convergence.\n",
        "By adding the square of the current gradient to the accumulated squared\n",
        "gradients, the algorithm keeps track of the historical gradients for each parameter.\n",
        "This leads to larger updates for infrequent parameters and smaller\n",
        "updates for frequent parameters.\n",
        "The purpose of squaring the gradients is to ensure that only positive values\n",
        "contribute to the accumulation. This helps Adagrad to adapt the learning rate\n",
        "more appropriately to different parameters, leading to better convergence in\n",
        "non-convex optimization problems.\n",
        "'''\n",
        "\n",
        "    # Update parameters using Adagrad\n",
        "    w -= (lr / np.sqrt(gw_accum + epsilon)) * dw  # Update weight using Adagrad\n",
        "    b -= (lr / np.sqrt(gb_accum + epsilon)) * db  # Update bias using Adagrad\n",
        "\n",
        "'''\n",
        "These two lines of code are performing parameter updates using the Adagrad\n",
        "optimization algorithm. Adagrad is an adaptive learning rate algorithm that\n",
        "adapts the learning rate for each parameter based on the historical gradients\n",
        "for that parameter.\n",
        "The update equations are as follows:\n",
        "w -= (learning_rate / sqrt(accumulated_gradient_w + epsilon)) * gradient_w\n",
        "b -= (learning_rate / sqrt(accumulated_gradient_b + epsilon)) * gradient_b\n",
        "In these lines of code:\n",
        "- \"w\" and \"b\" are the weight and bias parameters of the model, respectively.\n",
        "- \"lr\" is the learning rate, controlling the size of the updates.\n",
        "- \"gw_accum\" and \"gb_accum\" are the accumulated squared gradients for\n",
        "the weight and bias, respectively.\n",
        "- \"epsilon\" is a small constant added to the denominator to prevent division by zero.\n",
        "\n",
        "Overall, these lines of code update the weight and bias parameters using\n",
        "the Adagrad optimization algorithm to improve the model's performance during training\n",
        "'''\n",
        "\n",
        "# Output the final parameters\n",
        "print(f\"Final weight (w): {w:.4f}\")  # Print final weight with 4 decimal places\n",
        "print(f\"Final bias (b): {b:.4f}\")  # Print final bias with 4 decimal places"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtS3H5gt0mY4",
        "outputId": "d4b885cd-cf43-4ca9-e111-0608bc96e8d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weight (w): 1.5788\n",
            "Final bias (b): 0.5605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIO_NZpO0eKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}